# Workflow name as displayed on GitHub Actions
name: Update Diarize-CLI Sidecar

# Permissions granted to the GITHUB_TOKEN for this workflow
permissions:
  contents: write # Allows the workflow to write to the repository (e.g., create releases and upload assets)

# Triggers for the workflow
on:
  workflow_dispatch:
    inputs:
      release_tag:
        description: 'Git tag to use for the release (must already exist)'
        required: true
  workflow_call:
    inputs:
      release_tag:
        description: 'Git tag to use for the release (must already exist)'
        required: true
        type: string

jobs:
  build-diarize-matrix:
    # Specifies the runner environment for the job
    # Uses a matrix strategy to run on different operating systems
    runs-on: ${{ matrix.os }}
    
    # Environment variables available to all steps in this job
    env:
      HUGGINGFACE_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }} # Securely pass Hugging Face token
      RELEASE_TAG: ${{ inputs.release_tag }}
      PYTHONIOENCODING: "UTF-8" # Fix for UnicodeEncodeError on Windows
      
    strategy:
      matrix:
        include:
          - os: macos-13
            folder: macos-x86_64
          - os: macos-14
            folder: macos-arm64
          - os: windows-latest
            folder: windows-x86_64
          - os: windows-latest
            folder: windows-arm64
          - os: ubuntu-latest
            folder: linux-x86_64
          - os: ubuntu-latest
            folder: linux-arm64
            
    defaults:
      run:
        shell: bash

    steps:
      # Step 1: Checkout the repository code
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          persist-credentials: true # Persist credentials for git push
          fetch-depth: 0          # Fetch all history for all branches and tags

      # Step 2 [QEMU]: Set up QEMU for ARM64 emulation on x86 runner
      - name: Set up QEMU
        if: matrix.folder == 'linux-arm64'
        uses: docker/setup-qemu-action@v3

      # Step 2 [NATIVE]: Set up Python 3.11
      - name: Set up Python 3.11
        if: matrix.folder != 'linux-arm64'
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install FFmpeg (macOS)
        if: runner.os == 'macOS'
        run: brew install ffmpeg

      # The following native steps are skipped for the ARM build
      - name: Install dependencies
        if: matrix.folder != 'linux-arm64'
        run: |
          python -m pip install --upgrade pip
          pip install "numpy<2.0"
          pip install pyannote.audio huggingface_hub pyinstaller matplotlib lightning_fabric speechbrain
          if [[ "${{ runner.os }}" == "Linux" || "${{ runner.os }}" == "Windows" ]]; then
            pip install torch==2.3.0+cpu torchvision==0.18.0+cpu torchaudio==2.3.0+cpu --index-url https://download.pytorch.org/whl/cpu
          else # macOS
            pip install torch torchvision torchaudio
          fi

      - name: Log PyTorch/Torchaudio installation sizes (Linux only)
        if: runner.os == 'Linux' && matrix.folder != 'linux-arm64'
        run: |
          echo "--- Size of site-packages/torch ---"
          du -sh $(python3 -c "import torch, os; print(os.path.dirname(torch.__file__))")
          echo "--- Size of site-packages/torchaudio ---"
          du -sh $(python3 -c "import torchaudio, os; print(os.path.dirname(torchaudio.__file__))")

      - name: Log in to Hugging Face
        if: matrix.folder != 'linux-arm64'
        run: hf auth login --token "$HUGGINGFACE_TOKEN"

      - name: Preload Pyannote pipeline and copy models
        if: matrix.folder != 'linux-arm64'
        run: |
          # Preload speaker diarization pipeline into cache
          python3 - << 'EOF'
          import os
          from pyannote.audio import Pipeline
          Pipeline.from_pretrained(
              "pyannote/speaker-diarization-3.1"
          )
          EOF

          # Define Pyannote cache directory
          HF_CACHE_DIR="${HOME}/.cache/torch/pyannote"
          
          # Copy models to project folder
          mkdir -p diarize-cli/models
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            for d in "$HF_CACHE_DIR"/models--pyannote--*; do
              [ -d "$d" ] && cp -LR "$d" diarize-cli/models/
            done
          else
            for d in "$HF_CACHE_DIR"/models--pyannote--*; do
              [ -d "$d" ] && rsync -aL "$d" diarize-cli/models/
            done
          fi

          echo "--- Size of diarize-cli/models (Linux only) ---"
          if [[ "${{ runner.os }}" == "Linux" ]]; then
            du -sh diarize-cli/models
          fi

      - name: Generate Matplotlib font cache
        if: matrix.folder != 'linux-arm64'
        run: |
          python3 - << 'EOF'
          import os, matplotlib as mpl
          from matplotlib import font_manager as fm
          # Remove stale cache files
          for fname in ("fontList.json","fontList.py3k.cache","fontlist-v330.json"):
              p = mpl.get_cachedir() + "/" + fname
              if os.path.exists(p):
                  os.remove(p)
          fm.findSystemFonts()
          _ = fm.fontManager
          EOF

      - name: Copy Matplotlib cache to resources
        if: matrix.folder != 'linux-arm64'
        run: |
          mkdir -p diarize-cli/resources/.matplotlib
          MATPLOTLIB_CACHE_DIR=$(python3 -c 'import matplotlib as mpl; print(mpl.get_cachedir())')
          if [ -d "$MATPLOTLIB_CACHE_DIR" ]; then
            if [[ "${{ runner.os }}" == "Windows" ]]; then
              cp -R "$MATPLOTLIB_CACHE_DIR/" diarize-cli/resources/.matplotlib/
            else
              rsync -av --exclude='*.lock' "$MATPLOTLIB_CACHE_DIR/" diarize-cli/resources/.matplotlib/
            fi
          fi

          echo "--- Size of diarize-cli/resources/.matplotlib (Linux only) ---"
          if [[ "${{ runner.os }}" == "Linux" ]]; then
            du -sh diarize-cli/resources/.matplotlib
          fi

      - name: Locate Lightning-Fabric and SpeechBrain paths
        if: matrix.folder != 'linux-arm64'
        run: |
          LIGHT_INFO=$(python3 -c "import lightning_fabric, os; print(os.path.dirname(lightning_fabric.__file__))")
          SPEECH_DIR=$(python3 -c "import speechbrain, os; print(os.path.dirname(speechbrain.__file__))")
          echo "LIGHT_INFO=$LIGHT_INFO" >> $GITHUB_ENV
          echo "SPEECH_DIR=$SPEECH_DIR" >> $GITHUB_ENV

      - name: Build with PyInstaller
        if: matrix.folder != 'linux-arm64'
        run: |
          pyinstaller --clean --onefile \
            --name diarize-cli \
            --add-data "diarize-cli/models:models" \
            --add-data "diarize-cli/resources/.matplotlib:resources/.matplotlib" \
            --add-data "${LIGHT_INFO}:lightning_fabric" \
            --add-data "${SPEECH_DIR}:speechbrain" \
            --hidden-import=pyannote.audio.pipelines \
            --hidden-import=pyannote.audio.models \
            --hidden-import=pyannote.audio.models.segmentation \
            --hidden-import=pyannote.audio.models.embedding \
            diarize-cli/diarize_cli.py

          echo "--- Size of dist directory (Linux only) ---"
          if [[ "${{ runner.os }}" == "Linux" ]]; then
            du -sh dist
            ls -lh dist
          fi

      # Step 3-9 [QEMU]: The entire build process inside an ARM64 container
      - name: Build for linux-arm64 using QEMU
        if: matrix.folder == 'linux-arm64'
        run: |
          docker run --rm --platform linux/arm64 -v "${{ github.workspace }}:/app" -w /app \
            -e HUGGINGFACE_TOKEN \
            arm64v8/python:3.11 \
            sh -c ' \
            # Step: Install system dependencies
            apt-get update && apt-get install -y rsync && \
            \
            # Step: Install Python dependencies
            pip install --upgrade pip && \
            pip install "numpy<2.0" && \
            pip install pyannote.audio huggingface_hub pyinstaller matplotlib lightning_fabric speechbrain && \
            pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cpu && \
            \
            # Step: Log in
            hf auth login --token "$HUGGINGFACE_TOKEN" && \
            \
            # Step: Preload and copy models
            python3 -c "from pyannote.audio import Pipeline; Pipeline.from_pretrained('pyannote/speaker-diarization-3.1')" && \
            HF_CACHE_DIR="/root/.cache/torch/pyannote" && \
            mkdir -p diarize-cli/models && \
            for d in "$HF_CACHE_DIR"/models--pyannote--*; do [ -d "$d" ] && rsync -aL "$d" diarize-cli/models/; done && \
            du -sh diarize-cli/models && \
            \
            # Step: Generate and copy matplotlib cache
            python3 -c "import os, matplotlib as mpl; from matplotlib import font_manager as fm; fm.findSystemFonts()" && \
            mkdir -p diarize-cli/resources/.matplotlib && \
            MATPLOTLIB_CACHE_DIR=$(python3 -c "import matplotlib as mpl; print(mpl.get_cachedir())") && \
            rsync -av --exclude='*.lock' "$MATPLOTLIB_CACHE_DIR/" diarize-cli/resources/.matplotlib/ && \
            du -sh diarize-cli/resources/.matplotlib && \
            \
            # Step: Locate paths and build
            LIGHT_INFO=$(python3 -c "import lightning_fabric, os; print(os.path.dirname(lightning_fabric.__file__))") && \
            SPEECH_DIR=$(python3 -c "import speechbrain, os; print(os.path.dirname(speechbrain.__file__))") && \
            pyinstaller --clean --onefile \
              --name diarize-cli \
              --add-data "diarize-cli/models:models" \
              --add-data "diarize-cli/resources/.matplotlib:resources/.matplotlib" \
              --add-data "${LIGHT_INFO}:lightning_fabric" \
              --add-data "${SPEECH_DIR}:speechbrain" \
              --hidden-import=pyannote.audio.pipelines \
              --hidden-import=pyannote.audio.models \
              --hidden-import=pyannote.audio.models.segmentation \
              --hidden-import=pyannote.audio.models.embedding \
              diarize-cli/diarize_cli.py && \
            \
            # Step: Log final size
            du -sh dist && \
            ls -lh dist && \
            \
            # Step: Fix permissions for files created by root in container
            chown -R $(stat -c %u:%g .) dist build \
            '

      # Step 10: Prepare assets for upload by renaming the binary
      - name: Prepare assets for upload
        run: |
          mkdir -p upload_assets
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            mv dist/diarize-cli.exe upload_assets/diarize-cli-${{ matrix.folder }}.exe
          else
            mv dist/diarize-cli upload_assets/diarize-cli-${{ matrix.folder }}
          fi
          
      # Step 11: Ensure GitHub release exists (create if missing)
      - name: Ensure GitHub release
        id: ensure_release
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          result-encoding: string
          script: |
            const tag = process.env.RELEASE_TAG;
            try {
              const rel = await github.rest.repos.getReleaseByTag({
                owner: context.repo.owner,
                repo: context.repo.repo,
                tag
              });
              console.log(`Found existing release for tag ${tag}`);
              return rel.data.upload_url;
            } catch (e) {
              if (e.status === 404) {
                console.warn(`Release ${tag} not found, creating it.`);
                const rel = await github.rest.repos.createRelease({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  tag_name: tag,
                  name: `Harvey Sidecars ${tag}`,
                  body: `Automated build of diarize-cli sidecars. Release tag: ${tag}. Commit: ${context.sha}`,
                  draft: false,
                  prerelease: false
                });
                console.log(`Created new release for tag ${tag}`);
                return rel.data.upload_url;
              } else {
                console.error(`Error checking release: ${e.message}`);
                throw e;
              }
            }

      # Step 12: Upload assets
      - name: Upload assets
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          cd upload_assets
          ASSET_FILE=$(ls)
          echo "Preparing to upload: $ASSET_FILE"
          
          # Create checksum file
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            sha256sum "$ASSET_FILE" > "${ASSET_FILE}.sha256"
          else
            shasum -a 256 "$ASSET_FILE" > "${ASSET_FILE}.sha256"
          fi
          
          # Upload both the binary and its checksum file
          gh release upload ${{ env.RELEASE_TAG }} "$ASSET_FILE" "${ASSET_FILE}.sha256" --clobber
